---
title: "Stat596 FinalProject"
author: "Brianna Eskin"
date: "12/4/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**I: Introduction**

|    Every four years on the first Tuesday in November, the United States participates in the election of a new President. The years and months leading up to the election are filled with potential candidates working to persuade voters that they are the best choice for the role, and media outlets focus a lot of time and money into predicting how voters will cast their votes. This is usually done via polling potential voters to get an idea of how they plan on voting. While pre-election polling was generally trusted among the American people, their validity was called into question after the 2016 election victory of Donald Trump, which was considered a major upset, with Hillary Clinton preicted to win in many of the polls, some as confident as 99% in a Clinton victory. Many polling sources were forced to revisit their polling practices and try to understand what went wrong. According to Pew Research, inaccuracies in the polling stemmed from **a)** the demographics of those who chose to respond to the polling surveys in the first place, with many being college educated voters who tend to vote Democrat, **b)** the quality of the surveys when compared to the sample size, **c)** inaccurate prediction of where the "Undecided" voters would end up voting, which broke heavily in 2016 for Donald Trump (aka the "shy Trump voter"), and **d)** the true margin of error in polls being grossly understated, with the real margin of error predicted to be closer to 6 points instead of the standard 3 points **(add citation here)**. Despite several improvements made to polling techniques ahead of the 2020 election, there were still many states where the polling data showed very different results from the actual election. The general pattern was pretty much the same, with the polls underestimating support for Donald Trump. Take for example the key battleground states of Wisconsin and Michigan, which while correctly predicted that Biden would win the state, overstimated his support by 7 and 5 points repesctively. Probably the largest culprit, Florida was predicted to be a Biden win by 3 points, but instead lost the election by 3 points **(add citation here)**.
|    This paper will look into a potential election prediction model that does not take personal response into account. Instead, we will look at some key demographics at the county level across the United States to see if we can accurately predict how people will vote purely based on the makeup of the population. We will be able to confirm if certain stereotypes about populations hold, such as the voting tendencies of older Americans to lean towards Republican or more densely populated areas to vote Democrat. If this general model is successful, we can apply it to future elections without the unexpected biasness and variability that comes with relying on human response, and restore the public faiths in pre-election and post-election results.


**II: Datasets**

|    For this analysis, we will focus on the 1992 Presidential election between incumbent George H.W. Bush and winner Bill Clinton. More specifically, we will focus on the percentage of people who voted for Bill Clinton on the county level across all counties in the United States. This data was compiled by Professor Larry Winner from the University of Florida, based on data gathered from the U.S. Census Bureau **(add citation here)**. The response variable will be the percentage of people who voted for Bill Clinton in the election. There are nine predictors variables in our data: Median Age, Mean Savings, PerCapita Income, Percent of People Living in Poverty, Veteran Population Percentage, Female Population Percentage, Population Denisty (per square mile), Percentage of Population in a Nursing Home, and the Crime Index per Capita. All percentages are assumed to be between 0-100. There are N=2704 county observations in the dataset. All data is accounted for in the csv. A copy of the csv, as well as the code written to conduct this analysis, can be found on the Github account https://github.com/briannaeskin/RegressionAndTimeSeriesFall2020.


**III: Data Analysis and Model Selection**

|    We applied multiple linear regression techniques on our dataset to identify the minimum number of predictors required to explain the variability in the election results. Using all of the available data points, we first fit all of the predictors to a linear model with no polynomials, and calculated the VIF for each variable to check for potential collinearity between the predictors. Then, we checked the residuals to confirm normality $\epsilon \sim N(0,\sigma^2)$. If the assumption was violated, we checked for potential transformations using the Box-Cox plot. Next, we looked for potential outliers and leverage points and removed points with large leverage by using visual techniques, as well as calculation of the jackknife residuals and Cook's Distance. Once influential points were removed from the model, I repeated all prior analysis to check if the model was severly impacted by the influential points. Then, using BIC as the filtering critera, I performed backwards model selection and selected the number of predictors that generated the minimum BIC. Once the smaller model was generated, I repeated all previous analysis as a way to confirm that any irregularities in the model to this point were not due to unecessary predictors.